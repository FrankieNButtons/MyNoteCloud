### 问题一：SSHSCP投放authorized_keys文件不通
1. 首先应尝试关闭或放行防火墙：
```bash
sudo ufw disable
sudo ufw allow ssh
sudo ufw allow scp
sudo ufw status
```
2. 其次要检查目标路径是否为SSH连接用户可修改的路径：
```bash
sudo chmod -R 755 /app
```

### 问题二：scp投放authorized_keys文件成功，但投放/spark-3.3.0文件夹时失败
#### 解决方案1：更改SSH登陆权限（老师手册方案，失败）
1. 在投送目标机器上执行：
```bash
sudo nano /etc/ssh/sshd_config
```
2. 将`PermitRootLogin prohibit-password`改为`PermitRootLogin yes`


### 问题三：更改静态IP后无法联网的问题
#### 解决方案
1. 检查是否更改了`编辑->虚拟网络编辑器->NAT设置`中的子网网段指目标网断并指定了合适的网关（一般虚拟网卡为`VMnet8`）
2. 检查Ubuntu虚拟机中的静态地址是否与网卡在同一个网段
3. **千万千万不要忘了手动配置DNS服务器地址为网关地址**（否则无法上网）

### 问题四：由于自行配置了DNS服务器IP导致`/etc/hosts`解析优先级下降（可能）导致无法通过主机名解析IP
#### 解决方案
# 神经呀，根本不是这个问题！！！！！！！！
# **原因是`etc/hosts`中的IP和主机名必须要用空格而不能用制表符隔开······**
要求格式：
```plaintext
192.168.38.129  hadoop1 
192.168.38.130  hadoop2 
192.168.38.131  hadoop3

127.0.0.1       localhost
```
而不能是：
```plaintext
192.168.38.129\thadoop1 
192.168.38.130\thadoop2 
192.168.38.131\thadoop3

127.0.0.1\tlocalhost
```
而二者的显示都是：
```plaintext
192.168.38.129  hadoop1 
192.168.38.130  hadoop2 
192.168.38.131  hadoop3

127.0.0.1       localhost
```
**所以根本看不出来是`etc/hosts`的问题！！！！！！！！**

### 许久没打开的虚拟机启动后无法联网
#### 解决方案
重启网口即可：
```bash
sudo nmcli network on
```
