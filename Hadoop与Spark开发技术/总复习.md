# 第一讲：大数据概论
## 课件批注
大数据计算的四种主要形式及其常用框架：
![计算模式](images/image.png)
## 知识笔记
1. `Apache Flink`是由Apache软件基金会开发的开源流处理框架，其核心是用`Java和Scala`编写的分布式流数据流引擎。
2. Spark和Flink发展至今，基本上已经是实时计算领域的事实标准。
3. `大数据`、`云计算`、`物联网`是IT领域的最新发展趋势。
4. Nvidia、阿里巴巴等公司正在加紧开发Spark对GPU的支持。
# 第二讲 大数据存储架构Hadoop
## 课件批注
Google为Hadoop提供了灵感来源：
![Google Inovated](images/image-1.png)
HDFS、YARN、MapReduce的关系：
![关系图](images/image-2.png)
Hadoop生态系统概览：
![Hadoop生态系统概览](images/image-3.png)

## 知识笔记
### 历史事实
1. Gogg Cutting -开发-> Lucene -子项目-> Nutch -扩展后并入-> Hadoop --> 并入Apache基金会
### **Hadoop**
1. Hadoop Distributed File System，简称HDFS，是一个分布式文件系统。
    - NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。
    - DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。
    - Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。**通常不放在NameNode主机中，放在一个独立的机器上**。
2. Yet Another Resource Negotiator，简称YARN，是Hadoop的资源管理器。
    - ResourceManager(rm)：负责集群的资源调度，管理集群的资源（内存、CPU等）。
    - NodeManager(nm)：负责管理单个节点的资源，例如启动容器，管理容器的资源。
    - ApplicationMaster(am)：负责管理单个任务。
    - Container：在NodeManager上运行的进程，负责执行任务相当于一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等资源。
3. MapReduce是Hadoop的分布式计算框架，将计算分为2个部分
    - Map：将输入数据按照一定的规则进行分组，每个分组就是一个key，key对应的value就是分组的数据。主要阶段有：Partition、Shuffle、Sort和
    - Reduce：对Map的结果进行合并汇总。
    - MapReduce的特点
         1. 易于编程
         2. 扩展能力强
         3. 高容错（HDFS备份机制）
         4. 适合PB级以上数据的**离线处理**
4. Hadoop项目结构的不同部分总结：
    - HDFS：Hadoop分布式文件系统，负责存储数据，管理数据，提供数据访问接口。
    - YARN：分布式计算框架，负责调度资源
    - ZooKeeper：分布式协调服务，负责管理集群状态，提供分布式锁。
    - Hbase：Hadoop分布式数据库，负责存储数据，管理数据，提供数据访问接口。
    - MapReduce：离线计算框架。
    - Tez：DAG计算框架，负责优化MapReduce的执行计划。
    - Spark：实时内存计算框架。
    - Flume：一个专门用于日志收集和传输的分布式服务。Flume 的核心功能是可靠、高效地收集、聚合和传输数据（尤其是日志数据），它的目标是作为日志采集工具，为流计算框架提供数据来源。
    - sqoop：一个用于从各种数据源（如关系数据库、文件系统、HBase 等）到 Hadoop 集群中导入数据的`TEL工具`。
    - Oozie：一个用于管理和调度 Hadoop 集群上作业流的工具。
    - Ambari ：一个用于管理 Hadoop 集群的 Web UI。
![Hadoop项目结构](images/image-4.png)



# 第六讲Spark入门
## Scala语言入门
1. 数据类型介绍
    - `Byte`： 8位整数
    - `Short`： 16位整数
    - `Int`： 32位整数
    - `Long`： 64位整数
    - `Float`： 32位浮点数
    - `Double`： 64位浮点数
    - `Char`： 16位字符
2. 各种组合数据类型的索引方式

| 数据结构      | 索引方式                                  | 备注                   |
|---------------|-------------------------------------------|---------------------------------|
| **数组**      | 使用 `()` 访问，索引从 0 开始             | 索引越界抛出 `ArrayIndexOutOfBoundsException` |
| **列表**      | 使用 `apply` 或 `lift` 方法               | `lift` 返回 `Option` 类型，避免越界异常  |
| **集合**      | 无索引，使用迭代访问                      | 无序集合，无法通过索引直接访问元素      |
| **映射**      | 使用键（key）访问值（value）             | 索引越界会抛出异常，可用 `get` 和 `getOrElse` |
| **字符串**    | 使用 `()` 访问单个字符，索引从 0 开始    | 索引越界抛出 `StringIndexOutOfBoundsException` |
| **序列**      | 使用 `()` 访问，类似数组                  | 支持索引访问                             |
| **可变集合**  | 使用 `()` 访问和修改                      | 支持修改和索引访问                       |
| **元组**      | 使用 `._n` 或 `productElement(n)` 访问元素 | **索引是从 1 开始的**                        |
3. `to`包含上限，`until`不包含上限，如`val sq = Seq(1 to 5)`的结果为`Seq(1,2,3,4,5)`而`val sq = Seq(1 until 5)`的结果为`Seq(1,2,3,4)`。
4. Scala有个很特殊的方法`.foreach`，可以在参数中提供对于遍历值的回调函数以实现语法简化：
    - 设arr1是一个一维数组，用foreach遍历该数组每个元素的Scala语句`arr1.foreach(println)`或`arr1.foreach(x => println(x))`或`arr1.foreach(println(_))`。
    - 设arr2是一个二维数组，用foreach遍历该数组每个元素的Scala语句`arr2.foreach(x => x.foreach(println))`或`arr2.foreach(_.foreach(println))`或`arr2.foreach(x => x.foreach(println(_)))`。
5. Scala中`Nil`表示`长度为0得空列表List()`
6. 

## 数据结构简介
### RDD（resilient distributed dataset）
1. 具有以下特点：
    - 分布式数据集，可以处理大数据量
    - 弹性分布式数据集，可以自动容错
    - 不可修改

# 一些重点内容的表格汇总
## 一些常见管理工具的WebUI端口汇总

| **组件**          | **服务**                 | **默认端口号** |
|-------------------|--------------------------|----------------|
|                   | NameNode                | 9870           |
|  **HDFS**         | Secondary NameNode      | 9868           |
|                   | DataNode                | 9864           |
|                   | **ResourceManager**         | 8088           |
|  **YARN**         | **NodeManager**             | 8042           |
|                   | **JobHistory Server**       | 19888          |
|                   | Spark Master            | 8080           |
|  **Spark**        | Spark Worker            | 8081           |
|                   | Spark History Server    | 18080          |
| **Hive**          | HiveServer2             | 10000          |
| **HBase**         | HBase Master            | 16010          |
| **ZooKeeper**     | 客户端端口              | 2181           |
| **Flume**         | HTTP Source             | 41414          |
| **Oozie**         | Oozie Web UI            | 11000          |
| **Ambari**        | Ambari Server           | 8080           |
---
## 几个容易混淆的流处理框架与批处理框架的汇总

| 批处理框架名称      | 定性          | 简介 |
|---------------------|---------------|-------------------------------------------------------|
| Hadoop MapReduce    | 批处理框架    | 经典的分布式计算框架，用于处理大规模批量数据。         |
| Spark               | 批处理框架    | 内存计算为核心，支持批处理、流处理和机器学习等多种任务。 |
| Hive                | 数据仓库工具  | 基于 Hadoop 的数据仓库，支持 SQL 风格的批量数据查询。  |
| Tez                 | 批处理框架    | Hadoop 生态中的 DAG 执行引擎，用于替代 MapReduce。     |
| Pig                 | 数据流编程工具 | Hadoop 生态中的一种高级语言，用于批量数据处理。        |

---


| 流处理框架名称       | 定性          | 简介      |
|----------------------|---------------|------------------------------------------------------|
| Flume               | 数据收集工具  | 专注于日志和数据的收集与传输，不能直接进行流计算。    |
| Flink               | 流批一体框架    | 支持流批一体，且流处理性能出众，但最初擅长批处理，千万别和 Flume 搞混了！    |
| Kalka               | 流处理框架    | 分布式消息队列系统，主要用于数据传输和缓冲，不是专门的流计算框架。|
| Kafka Streams       | 流处理框架    | 基于 Kafka 的轻量级流处理库，支持实时数据流处理。     |
| Storm               | 流处理框架    | 分布式流计算框架，擅长低延迟、事件驱动的数据处理。    |
| Spark Streaming     | 微批处理框架  | Spark 的流处理组件，基于**微批次**实现流式数据处理。      |
| Pulsar Functions    | 流处理框架    | Apache Pulsar 中的轻量级流处理引擎，适用于简单任务。  |
| Beam                | 流批一体框架  | Apache Beam 提供统一的编程模型，支持多种后端执行引擎。|



# 总复习题汇总
1. Hadoop默认的块大小为`128MB`
2. Spark使用`Scala`语言开发
3. Hadoop使用`Java`语言开发
4. Hadoop的资源管理器是`Yarn`（全称为`Yet Another Resource Negotiator`）
5. Hadoop的分布式文件系统是`HDFS`
6. Hadoop的计算框架是`MapReduce`
7. Hadoop的基本计算单元是`Split`
8. MapReduce的Shuffle过程分为`Partition`、`Sort`和`Merge`三个阶段（也可以有第4个`Combine`阶段,其作用为`在Mapper端对数据进行局部聚合`）
9. Scala中的Unit类型相当于`void`类型
10. NameNode的作用是`协调集群中的数据存储（存储元数据）`
11. Scala中的数值类型总共有`Byte`，`Short`，`Int`，`Long`，`Float`，`Double`，`Char`
12. DataNode的作用是`存储拆分后的数据块`
13.  `val lst10 = List(1, 2, 3, 4, 5), println(lst10.foldLeft(0)((x, y) => x - y))`的结果为`-15`（0-1-2-3-4-5=-15）
14.  `val lst10 = List(1, 2, 3, 4, 5), println(lst10.foldRight(0)((x, y)=> x - y))`的结果为`3`（5-(4-(3-(2-(1-0))))=3）
15. Spark中RDD的创建方式有`Paralize`和`makeRDD`
16. Spark运行速度比MapReduce快的原因是`内存计算`和`DAG优化`
17. `JobTracker`是`MapReduce`中的一个核心组件，作用是`协调数据计算任务`
18. Spark的启动命令是`start-all.sh`
19.  MapReduce中Mapper执行map方法的次数等于Split的`行数`
20. RDD的全称是Resilient Distributed Dataset，即`弹性分布式数据集`
21. HDFS的最小存储单位叫`Block`
22. MapReduce适合PB级以上数据的`离线`处理
23. PySpark使用RDD从整数数组`rdd = sc.paralize([10, 27, 65, 22, 82, 86, 75])`取出偶数并乘以2：`rdd.filter(lambda x: x % 2 == 0).map(lambda x: x * 2).collect()`
24. 21的功能使用Scala实现是`val rdd = sc.paralize([10, 27, 65, 22, 82, 86, 75])`后`val result = rdd.filter(x => x % 2 == 0).map(x => x * 2).collect()`
25. 在一个数组`Array arr`中筛出偶数，并乘以2的代码是`arr.filter(_ % 2 == 0).map(_ * 2)`
26. Scala中要输出元组`tuple`中的第四个值，输出语句为`tuple._4`
27. RDD与其父RDD有`窄依赖`和`宽依赖`两种依赖
28. Spark Streaming使用`微批次`的架构把流处理变为连续小规模批处理。
29. SparkSQL中新增了两种数据抽象`DataFrame``和`DataSet`。
30. Storm的开发语言是`Clojure`
31. Kalfka集群包含一个或多个服务器，这种服务器被称为`Broker`
32. 分布式消息系统有两种主要的信息传递模式`点对点`和`发布-订阅`（**其中Kafka消息系统是一种`发布-订阅模式`**）
33. Spark最新的SQL查询起始点是`SparkSession`，其内部封装了`SparkContext`
34. Scala-Shell中直接对DataFrame执行隐式的向RDD的转换需要导入spark中的`implicits._`模块（`import spark.implicits._`）
35. Hadoop中类`TextInputFormat`是默认的FileInputFormat实现类。执行读取每条记录，键是`该行在整个文件中的起始字节偏移量`，为`LongWritable`类型。值是`该行的内容`，为`Text`类型。
36. Spark有`Standalone`、`Spark on YARN`和`Spark on Mesos`三种部署方式
37. 使用Spark开发者需要便携一个程序`Driver`被提交到集群调度运行Worker，在其中定义了一系列多个RDD，冰雕用RDD上的`Action`
38. Spark的驱动器是执行开发程序中的main方法的进程。它负责开发人员编写的用来创建`SparkContext`对象、创建RDD，以及进行RDD的转换操作和行动操作代码的执行。如果你是用spark shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作sc的`SparkContext`的对象。如果驱动器程序终止了，那么Spark应用也就结束了。
38. Spark的`Executor`是一个工作进程，负责在Spark作业中运行任务，任务间相互独立。Spark应用启动时，该节点被同时启动，并且始终伴随着整个Spark应用的生命周期而存在。
39. 虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间选代型应用来说，随着迭代的进行RDDS之间的血缘关系会越来越长，一旦在后续选代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持`Checkpoint`将数据保存到持久化的存储中，这样就可以切断之前的血缘关系因为它之后的RDD不需要知道它的父RDDs了，它可以`Checkpoint`从处拿到数据。
40. DAG(Directed Acyclic craph)叫做`有向无环图`，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于宽依赖，由于有shuffle的存在，只能在父RDD处理完成后，才能开始接下来的计算，因此`宽依赖`是划分Stage的依据。
41. RDDS之间维护着这种血缘大系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是`一一对应`的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是`多对多`的关系。
42. spark对丁频繁使用的RDD，可以使用`缓存（Cache）`成`检查点（Checkpoint）`将其保存在内存或磁盘中，避免重复计算。（其中Cache保留了血缘关系，可以重新计算保存的任意RDD，而Checkpoint不保留血缘关系，可直接访问保存的RDD，推测缓存的血缘关系以保证其稳定性）。
43. 窄依赖是`父RDD的每个分区最多只会被子RDD的一个分区使用`，而宽依赖通常会触发`Shuffle`操作，可能导致性能瓶颈和容错下降。
44. Spark集群的主节点是`Master`，负责协调整个集群的资源分配和4任务调度。它接收来自客户端提交的应用程序，并为这些应用程序分配资源。
45. `Worker`是spark集群中的工作节点，负责执行实际的计算任务。可以启动多个`Executor`进程，这些进程用于执行任务。
46. `Spark Core`实现了Spark的基本功能，包含任务调度、内存理、错误恢复、与存储系统交互等模块。它还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义。
47. `SparkSession`是spark最新的SOL查询起始点，实质上是`SQLContext`和`HiveContext`的组合。（也包含`SparkContext`）。
48. Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如:`Kafka`、`Flume`、`HDFS`、`ZeroMQ`和`简单的ICP套接字`等。


# 选择题知识点展开
46. HDFS的Block默认副本数是`3`
47. `DataNode`负责HDFS的数据存储
48. SecondaryNameNode辅助NameNode的HDFS的元数据处理和保存（**主要作用是合并fsimage和editlog**）
49. NameNode HA是NameNode的高可用，NameNode HA的实现是Active/Standby的NameNode备份机制。与SecondaryNameNode无关。
50. Hadoop Client端上传文件是的流程是`直接在Client端切分为Blocks后依次上传文件到HDFS`。
51. `JobTracker`通常与`NameNode`在同一个节点启动。
52. `var t = List(1, 2, 3, 5, 5), println(t.reduce(_-_))`的输出值为`-14`(1-2-3-5-5=-14)
53. 与`Spark Streaming`相似的流计算组件是`Storm`
54. Kalka分布式消息队列系统，主要用于数据传输和缓冲，不是专门的流计算框架。但也有`Kafka Streams`基于Kafka实现了流处理计算。
55. Scala是强类型语言，`val b:String = 3`会报错。
56. 一些关于Spark的事实
    - Action操作触发执行
    - RDD由一系列partitiom组成
    - transformations操作延迟执行
    - 每个RDD必然与其它RDD存在依赖关系