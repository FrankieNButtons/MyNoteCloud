# 第一讲：大数据概论
## 课件批注
大数据计算的四种主要形式及其常用框架：
![计算模式](images/image.png)
## 知识笔记
1. `Apache Flink`是由Apache软件基金会开发的开源流处理框架，其核心是用`Java和Scala`编写的分布式流数据流引擎。
2. Spark和Flink发展至今，基本上已经是实时计算领域的事实标准。
3. `大数据`、`云计算`、`物联网`是IT领域的最新发展趋势。
4. Nvidia、阿里巴巴等公司正在加紧开发Spark对GPU的支持。
# 第二讲 大数据存储架构Hadoop
## 课件批注
Google为Hadoop提供了灵感来源：
![Google Inovated](images/image-1.png)
HDFS、YARN、MapReduce的关系：
![关系图](images/image-2.png)
Hadoop生态系统概览：
![Hadoop生态系统概览](images/image-3.png)

## 知识笔记
### 历史事实
1. Gogg Cutting -开发-> Lucene -子项目-> Nutch -扩展后并入-> Hadoop --> 并入Apache基金会
### **Hadoop**
1. Hadoop Distributed File System，简称HDFS，是一个分布式文件系统。
    - NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。
    - DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。
    - Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。**通常不放在NameNode主机中，放在一个独立的机器上**。
    - JobTracker(jt)：负责集群的调度，管理集群的资源（内存、CPU等）,通常会独立放在一个主机上，也可以和`NameNode`放在同一主机中。
    - TaskTracker(tt)：负责运行单个任务，管理任务运行所需要的资源，例如内存、CPU、磁盘、网络等资源，通常在每个`DataNode`都会运行一个，，负责执行任务。
2. Yet Another Resource Negotiator，简称YARN，是Hadoop的资源管理器。
    - ResourceManager(rm)：负责集群的资源调度，管理集群的资源（内存、CPU等）。
    - NodeManager(nm)：负责管理单个节点的资源，例如启动容器，管理容器的资源。
    - ApplicationMaster(am)：负责管理单个任务。
    - Container：在NodeManager上运行的进程，负责执行任务相当于一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等资源。
3. MapReduce是Hadoop的分布式计算框架，将计算分为2个部分
    - Map：将输入数据按照一定的规则进行分组，每个分组就是一个key，key对应的value就是分组的数据。主要阶段有：`Input/Spliting`、`Mapping`和`Shuffle`，其中`Shuffle`又可分为`Partition`、`Sort`和`Merge`，部分情况下也可以引入`Combine`在Mapper中完成对部分数据的局部聚合（分块的Reduce）。
    - Reduce：对Map的结果进行合并汇总。
    - MapReduce的特点
         1. 易于编程
         2. 扩展能力强
         3. 高容错（HDFS备份机制）
         4. 适合PB级以上数据的**离线处理**
4. Hadoop项目结构的不同部分总结：
    - HDFS：Hadoop分布式文件系统，负责存储数据，管理数据，提供数据访问接口。
    - YARN：分布式计算框架，负责调度资源
    - ZooKeeper：分布式协调服务，负责管理集群状态，提供分布式锁。
    - Hbase：Hadoop分布式数据库，负责存储数据，管理数据，提供数据访问接口。
    - MapReduce：离线计算框架。
    - Tez：DAG计算框架，负责优化MapReduce的执行计划。
    - Spark：实时内存计算框架。
    - Flume：一个专门用于日志收集和传输的分布式服务。Flume 的核心功能是可靠、高效地收集、聚合和传输数据（尤其是日志数据），它的目标是作为日志采集工具，为流计算框架提供数据来源。
    - sqoop：一个用于从各种数据源（如关系数据库、文件系统、HBase 等）到 Hadoop 集群中导入数据的`TEL工具`。
    - Hive：一个用于查询和操作 Hadoop 数据库的 SQL 引擎。
    - Oozie：一个用于管理和调度 Hadoop 集群上作业流的工具。
    - Ambari ：一个用于管理和构建 Hadoop 集群的 Web UI。
![Hadoop项目结构](images/image-4.png)



# 第六讲Spark入门
## Scala语言入门
1. 数据类型介绍
    - `Byte`： 8位整数
    - `Short`： 16位整数
    - `Int`： 32位整数
    - `Long`： 64位整数
    - `Float`： 32位浮点数
    - `Double`： 64位浮点数
    - `Char`： 16位字符
2. 各种组合数据类型的索引方式

| 数据结构     | 索引方式                                   | 备注                                           |
| ------------ | ------------------------------------------ | ---------------------------------------------- |
| **数组**     | 使用 `()` 访问，索引从 0 开始              | 索引越界抛出 `ArrayIndexOutOfBoundsException`  |
| **列表**     | 使用 `apply` 或 `lift` 方法                | lift返回 `Option` 类型，避免越界异常           |
| **集合**     | 无索引，使用迭代访问                       | 无序集合，无法通过索引直接访问元素             |
| **映射**     | 使用键（key）访问值（value）               | 索引越界会抛出异常，可用 `get` 和 `getOrElse`  |
| **字符串**   | 使用 `()` 访问单个字符，索引从 0 开始      | 索引越界抛出 `StringIndexOutOfBoundsException` |
| **序列**     | 使用 `()` 访问，类似数组                   | 支持索引访问                                   |
| **可变集合** | 使用 `()` 访问和修改                       | 支持修改和索引访问                             |
| **元组**     | 使用 `._n` 或 `productElement(n)` 访问元素 | **索引是从 1 开始的**                          |
3. `to`包含上限，`until`不包含上限，如`val sq = Seq(1 to 5)`的结果为`Seq(1,2,3,4,5)`而`val sq = Seq(1 until 5)`的结果为`Seq(1,2,3,4)`。
4. Scala有个很特殊的方法`.foreach`，可以在参数中提供对于遍历值的回调函数以实现语法简化：
    - 设arr1是一个一维数组，用foreach遍历该数组每个元素的Scala语句`arr1.foreach(println)`或`arr1.foreach(x => println(x))`或`arr1.foreach(println(_))`。
    - 设arr2是一个二维数组，用foreach遍历该数组每个元素的Scala语句`arr2.foreach(x => x.foreach(println))`或`arr2.foreach(_.foreach(println))`或`arr2.foreach(x => x.foreach(println(_)))`。
5. Scala中`Nil`表示`长度为0得空列表List()`
6. 

## 数据结构简介
### RDD（resilient distributed dataset）
1. 具有以下特点：
    - 分布式数据集，可以处理大数据量
    - 弹性分布式数据集，可以自动容错
    - 不可修改

# 一些重点内容的表格汇总
## 一些常见管理工具的WebUI端口汇总

| **组件**      | **服务**              | **默认端口号** |
| ------------- | --------------------- | -------------- |
|               | NameNode              | 9870           |
| **HDFS**      | Secondary NameNode    | 9868           |
|               | DataNode              | 9864           |
|               | **ResourceManager**   | 8088           |
| **YARN**      | **NodeManager**       | 8042           |
|               | **JobHistory Server** | 19888          |
|               | Spark Master          | 8080           |
| **Spark**     | Spark Worker          | 8081           |
|               | Spark History Server  | 18080          |
| **Hive**      | HiveServer2           | 10000          |
| **HBase**     | HBase Master          | 16010          |
| **ZooKeeper** | 客户端端口             | 2181           |
| **Flume**     | HTTP Source           | 41414          |
| **Oozie**     | Oozie Web UI          | 11000          |
| **Ambari**    | Ambari Server         | 8080           |
---
## 几个容易混淆的流处理框架与批处理框架的汇总

| 批处理框架名称   | 定性           | 简介                                                     |
| ---------------- | -------------- | -------------------------------------------------------- |
| Hadoop MapReduce | 批处理框架     | 经典的分布式计算框架，用于处理大规模批量数据。           |
| Spark            | 批处理框架     | 内存计算为核心，支持批处理、流处理和机器学习等多种任务。 |
| Hive             | 数据仓库工具   | 基于 Hadoop 的数据仓库，支持 SQL 风格的批量数据查询。    |
| Tez              | 批处理框架     | Hadoop 生态中的 DAG 执行引擎，用于替代 MapReduce。       |
| Pig              | 数据流编程工具 | Hadoop 生态中的一种高级语言，用于批量数据处理。          |

---


| 流处理框架名称   | 定性         | 简介                                                                      |
| ---------------- | ------------ | ------------------------------------------------------------------------- |
| Flume            | 数据收集工具 | 专注于日志和数据的收集与传输，不能直接进行流计算。                        |
| Flink            | 流批一体框架 | 支持流批一体，且流处理性能出众，但最初擅长批处理，千万别和 Flume 搞混了！ |
| Kafka            | 流处理框架   | 分布式消息队列系统，主要用于数据传输和缓冲，不是专门的流计算框架。        |
| Kafka Streams    | 流处理框架   | 基于 Kafka 的轻量级流处理库，支持实时数据流处理。                         |
| Storm            | 流处理框架   | 分布式流计算框架，擅长低延迟、事件驱动的数据处理。                        |
| Spark Streaming  | 微批处理框架 | Spark 的流处理组件，基于**微批次**实现流式数据处理。                      |
| Pulsar Functions | 流处理框架   | Apache Pulsar 中的轻量级流处理引擎，适用于简单任务。                      |
| Beam             | 流批一体框架 | Apache Beam 提供统一的编程模型，支持多种后端执行引擎。                    |



# 总复习题汇总
1. Hadoop默认的块大小为`128MB`
2. Spark使用`Scala`语言开发
3. Hadoop使用`Java`语言开发
4. Hadoop的资源管理器是`Yarn`（全称为`Yet Another Resource Negotiator`）
5. Hadoop的分布式文件系统是`HDFS`
6. Hadoop的计算框架是`MapReduce`
7. MapReduce的基本计算单元是`Split`
8. MapReduce的Shuffle过程分为`Partition`、`Sort`和`Merge`三个阶段（也可以有第4个`Combine`阶段,其作用为`在Mapper端对数据进行局部聚合`）
9. Scala中的Unit类型相当于`void`类型
10. NameNode的作用是`协调集群中的数据存储（存储元数据）`
11. Scala中的数值类型总共有`Byte`，`Short`，`Int`，`Long`，`Float`，`Double`，`Char`
12. DataNode的作用是`存储拆分后的数据块`
13.  `val lst10 = List(1, 2, 3, 4, 5), println(lst10.foldLeft(0)((x, y) => x - y))`的结果为`-15`（0-1-2-3-4-5=-15）
14.  `val lst10 = List(1, 2, 3, 4, 5), println(lst10.foldRight(0)((x, y)=> x - y))`的结果为`3`（5-(4-(3-(2-(1-0))))=3）
15. Spark中RDD的创建方式有`Paralize`和`makeRDD`
16. Spark运行速度比MapReduce快的原因是`内存计算`和`DAG优化`
17. `JobTracker`是`MapReduce`中的一个核心组件，作用是`协调数据计算任务`
18. Spark的启动命令是`start-all.sh`
19.  MapReduce中Mapper执行map方法的次数等于Split的`行数`
20. RDD的全称是Resilient Distributed Dataset，即`弹性分布式数据集`
21. HDFS的最小存储单位叫`Block`
22. MapReduce适合PB级以上数据的`离线`处理
23. PySpark使用RDD从整数数组`rdd = sc.paralize([10, 27, 65, 22, 82, 86, 75])`取出偶数并乘以2：`rdd.filter(lambda x: x % 2 == 0).map(lambda x: x * 2).collect()`
24. 21的功能使用Scala实现是`val rdd = sc.paralize(Array(10, 27, 65, 22, 82, 86, 75))`后`val result = rdd.filter(x => x % 2 == 0).map(x => x * 2).collect()`
25. 在一个数组`Array arr`中筛出偶数，并乘以2的代码是`arr.filter(_ % 2 == 0).map(_ * 2)`
26. Scala中要输出元组`tuple`中的第四个值，输出语句为`tuple._4`
27. RDD与其父RDD有`窄依赖`和`宽依赖`两种依赖
28. Spark Streaming使用`微批次`的架构把流处理变为连续小规模批处理。
29. SparkSQL中新增了两种数据抽象`DataFrame`和`DataSet`。
30. Storm的开发语言是`Clojure`
31. Kalfka集群包含一个或多个服务器，这种服务器被称为`Broker`
32. 分布式消息系统有两种主要的信息传递模式`点对点`和`发布-订阅`（**其中Kafka消息系统是一种`发布-订阅模式`**）
33. Spark最新的SQL查询起始点是`SparkSession`，其内部封装了`SparkContext`
34. Scala-Shell中直接对DataFrame执行隐式的向RDD的转换需要导入spark中的`implicits._`模块（`import spark.implicits._`）
35. Hadoop中类`TextInputFormat`是默认的FileInputFormat实现类。执行读取每条记录，键是`该行在整个文件中的起始字节偏移量`，为`LongWritable`类型。值是`该行的内容`，为`Text`类型。
36. Spark有`Standalone`、`Spark on YARN`和`Spark on Mesos`三种部署方式
37. 使用Spark开发者需要便携一个程序`Driver`被提交到集群调度运行Worker，在其中定义了一系列多个RDD，冰雕用RDD上的`Action`
38. Spark的驱动器是执行开发程序中的main方法的进程。它负责开发人员编写的用来创建`SparkContext`对象、创建RDD，以及进行RDD的转换操作和行动操作代码的执行。如果你是用spark shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作sc的`SparkContext`的对象。如果驱动器程序终止了，那么Spark应用也就结束了。
38. Spark的`Executor`是一个工作进程，负责在Spark作业中运行任务，任务间相互独立。Spark应用启动时，该节点被同时启动，并且始终伴随着整个Spark应用的生命周期而存在。
39. 虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间选代型应用来说，随着迭代的进行RDDS之间的血缘关系会越来越长，一旦在后续选代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持`Checkpoint`将数据保存到持久化的存储中，这样就可以切断之前的血缘关系因为它之后的RDD不需要知道它的父RDDs了，它可以从`Checkpoint`处拿到数据。
40. DAG(Directed Acyclic craph)叫做`有向无环图`，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于宽依赖，由于有shuffle的存在，只能在父RDD处理完成后，才能开始接下来的计算，因此`宽依赖`是划分Stage的依据。
41. RDDS之间维护着这种血缘大系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是`一一对应`的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是`多对多`的关系。
42. spark对于频繁使用的RDD，可以使用`缓存（Cache）`或`检查点（Checkpoint）`将其保存在内存或磁盘中，避免重复计算。（其中Cache保留了血缘关系，可以重新计算保存的任意RDD，而Checkpoint不保留血缘关系，可直接访问保存的RDD，推测缓存的血缘关系以保证其稳定性）。
43. 窄依赖是`父RDD的每个分区最多只会被子RDD的一个分区使用`，而宽依赖通常会触发`Shuffle`操作，可能导致性能瓶颈和容错下降。
44. Spark集群的主节点是`Master`，负责协调整个集群的资源分配和任务调度。它接收来自客户端提交的应用程序，并为这些应用程序分配资源。
45. `Worker`是spark集群中的工作节点，负责执行实际的计算任务。可以启动多个`Executor`进程，这些进程用于执行任务。
46. `Spark Core`实现了Spark的基本功能，包含任务调度、内存理、错误恢复、与存储系统交互等模块。它还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义。
47. `SparkSession`是spark最新的SOL查询起始点，实质上是`SQLContext`和`HiveContext`的组合。（也包含`SparkContext`）。
48. Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如:`Kafka`、`Flume`、`HDFS`、`ZeroMQ`和`简单的ICP套接字`等。


# 选择题知识点展开
1. HDFS的Block默认副本数是`3`
2. `DataNode`负责HDFS的数据存储
3.  `SecondaryNameNode`辅助NameNode的HDFS的元数据处理和保存（**主要作用是合并fsimage和editlog**）
4.  `NameNode HA`是NameNode的高可用，NameNode HA的实现是Active/Standby的NameNode备份机制。与SecondaryNameNode无关。
5.  Hadoop Client端上传文件是的流程是`直接在Client端切分为Blocks后依次上传文件到HDFS`。
6.  `JobTracker`通常与`NameNode`在同一个节点启动。
7.  `TaskTracker`通常会在每个`DataNode`节点启动一个。
8.  `var t = List(1, 2, 3, 5, 5), println(t.reduce(_-_))`的输出值为`-14`(1-2-3-5-5=-14)
9.  与`Spark Streaming`相似的流计算组件是`Storm`
10. Kafka分布式消息队列系统，主要用于数据传输和缓冲，不是专门的流计算框架。但也有`Kafka Streams`基于Kafka实现了流处理计算。
11. Scala是强类型语言，`val b:String = 3`会报错。
12. 一些关于Spark的事实
    - `Action`操作触发执行
    - RDD由一系列`partition`组成
    - `transformations`操作延迟执行
    - 每个RDD必然与其它RDD存在依赖关系


# 判断题知识点展开
1. `Apache Kafka`是一个分布式流处理平台，最初设计用于构建实时数据管道和流应用。然而，它的功能远不止于实时数据处理，它还可以用于多种场景，包括但不限于：`实时数据处理`、`批处理`、`消息队列`等。
2. Hadoop`不支持`数据的随机读写
3. MapReduce的`Split`默认大小和HDFS的`Block`大小一致
4. Hadoop中DataNode会定期向`NameNode`发送`心跳`，向它报告自己的状态。
5. 对于“发布-订阅”消息系统中，发布者发送到`topic`时，只有订阅了该`topic`的订阅者才能收到消息。
6. Spark的每个`Stage`包含多个`Task`，每个`Task`由一个`Executor`执行，`Task`是Stage的子任务。
7. `hadoop-env.sh`需要指定Hadoop所使用的`JAVA_HOME`
8. Scala不允许`Static`成员存在。
10. MapReduce的输入数据集只能是`静态`的。
11. 



# 简答题示例
## 简述Spark RDD缓存（Cache）机制和检査点（Checkpoint）机制的区别和性能对比
1. Cache是轻量化保存RDD数据,可存储在内存和硬盘,是分散存储,设计上数据是不安全的(保留RDD血缘关系)，CheckPoint是重量级保存RDD数据,是集中存储,只能存储在硬盘(HDES)上,设计上是安全的(不保留RDD血缘关系)。
2. Cache性能更好,因为是分散存储，各个Executor并行执行，效率高,可以保存到内存中(占内存),更快；checkPoint比较慢,因为是集中存储,涉及到网络IO,但是存储到HDFS上更加安全(多副本)。
## 一个基本的Hadoop集群有哪些节点，各有哪些功能?
    - NameNode:负责协调集群中的数据存储
    - DataNode:存储被拆分的数据块
    - JobTracker:协调数据计算任务
    - TaskTracker:负责执行出JobTracker指派的任务
    - SecondaryNameNode:帮助NameNode收集文件系统运行的状态信息


## 简述MapReduce作业的推测执行机制
1. MapReduce作业完成时间取决于最慢的任务完成时间。
2. 发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度，为拖后腿任务启动一个备份任务，同时运行。
3. 谁先运行完，则采用谁的结果
4. 不能启用推测执行机制的情形：
    - 任务间存在严重的负载倾斜。
    - 特殊任务:比如任务向数据库中写数据。

## 




# 几个必须掌握的编程任务
## Java:MapReduce
### MapReduce实现WordCount
```java
// Mapper
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

}

// Reducer
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable value : values) {
            sum += value.get();
        }
        context.write(key, new IntWritable(sum));
    }
}

// Driver


```

### MapReduce实现PageRank
```java

```

## Python:Spark
### PySpark实现WordCount并保存到`WordCount.txt`
```python
from pyspark import SparkContext

# 初始化 SparkContext
sc = SparkContext(appName="PySparkWordCount")

# 读取文本文件
file_path = "words.txt"  # 替换为你的文件路径
lines = sc.textFile(file_path)

# 执行 WordCount
word_counts = (
    lines.flatMap(lambda line: line.split(" "))  # 按空格分割为单词
         .map(lambda word: (word, 1))           # 转换为 (word, 1) 键值对
         .reduceByKey(lambda a, b: a + b)       # 根据键（单词）聚合，统计次数
)

# 将结果收集到 driver 并打印
for word, count in word_counts.collect():
    print(f"{word}: {count}")

# 保存结果到文件
output_path = "word_count_result"  # 替换为你的输出路径
word_counts.saveAsTextFile(output_path)

# 停止 SparkContext
sc.stop()
```

### PySpark实现Order表单处理
```python
from pyspark import SparkContext
import json
from datetime import datetime

# 初始化 SparkContext
sc = SparkContext(appName="OrderAnalysis")

# 读取文件并解析每一行
orders_rdd = sc.textFile("./data/order.txt") \
    .flatMap(lambda line: line.split("|")) \
    .map(lambda x: json.loads(x))

# print(orders_rdd.collect())
# [{'id': 1, 'timestamp': '2019-05-08T01:03.00Z', 'category': '平板电脑', 'areaName': '北京', 'money': '1450'}, {'id': 2, 'timestamp': '2019-05-08T01:01.00Z', 'category': '手机', 'areaName': '北京', 'money': '1450'}, {'id': 3, 'timestamp': '2019-05-08T01:03.00Z', 'category': '手机', 'areaName': '北京', 'money': '8412'}, {'id': 4, 'timestamp': '2019-05-08T05:01.00Z', 'category': '电脑', 'areaName': '上海', 'money': '1513'}, {'id': 5, 'timestamp': '2019-05-08T01:03.00Z', 'category': '家电', 'areaName': '北京', 'money': '1550'}, {'id': 6, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '杭州', 'money': '1550'}, {'id': 7, 'timestamp': '2019-05-08T01:03.00Z', 'category': '电脑', 'areaName': '北京', 'money': '5611'}, {'id': 8, 'timestamp': '2019-05-08T03:01.00Z', 'category': '家电', 'areaName': '北京', 'money': '4410'}, {'id': 9, 'timestamp': '2019-05-08T01:03.00Z', 'category': '家具', 'areaName': '郑州', 'money': '1120'}, {'id': 10, 'timestamp': '2019-05-08T01:01.00Z', 'category': '家具', 'areaName': '北京', 'money': '6661'}, {'id': 11, 'timestamp': '2019-05-08T05:03.00Z', 'category': '家具', 'areaName': '杭州', 'money': '1230'}, {'id': 12, 'timestamp': '2019-05-08T01:01.00Z', 'category': '书籍', 'areaName': '北京', 'money': '5550'}, {'id': 13, 'timestamp': '2019-05-08T01:03.00Z', 'category': '书籍', 'areaName': '北京', 'money': '5550'}, {'id': 14, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '北京', 'money': '1261'}, {'id': 15, 'timestamp': '2019-05-08T03:03.00Z', 'category': '电脑', 'areaName': '杭州', 'money': '6660'}, {'id': 16, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '天津', 'money': '6660'}, {'id': 17, 'timestamp': '2019-05-08T01:03.00Z', 'category': '书籍', 'areaName': '北京', 'money': '9000'}, {'id': 18, 'timestamp': '2019-05-08T05:01.00Z', 'category': '书籍', 'areaName': '北京', 'money': '1230'}, {'id': 19, 'timestamp': '2019-05-08T01:03.00Z', 'category': '电脑', 'areaName': '杭州', 'money': '5551'}, {'id': 20, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '北京', 'money': '2450'}, {'id': 21, 'timestamp': '2019-05-08T01:03.00Z', 'category': '食品', 'areaName': '北京', 'money': '5520'}, {'id': 22, 'timestamp': '2019-05-08T01:01.00Z', 'category': '食品', 'areaName': '北京', 'money': '6650'}, {'id': 23, 'timestamp': '2019-05-08T01:03.00Z', 'category': '服饰', 'areaName': '杭州', 'money': '1240'}, {'id': 24, 'timestamp': '2019-05-08T01:01.00Z', 'category': '食品', 'areaName': '天津', 'money': '5600'}, {'id': 25, 'timestamp': '2019-05-08T01:03.00Z', 'category': '食品', 'areaName': '北京', 'money': '7801'}, {'id': 26, 'timestamp': '2019-05-08T01:01.00Z', 'category': '服饰', 'areaName': '北京', 'money': '9000'}, {'id': 27, 'timestamp': '2019-05-08T01:03.00Z', 'category': '服饰', 'areaName': '杭州', 'money': '5600'}, {'id': 28, 'timestamp': '2019-05-08T01:01.00Z', 'category': '食品', 'areaName': '北京', 'money': '8000'}, {'id': 29, 'timestamp': '2019-05-08T02:03.00Z', 'category': '服饰', 'areaName': '杭州', 'money': '7000'}]
area_rdd = orders_rdd.map(lambda x: (x['areaName'],1))
# print(area_rdd.collect())
# [('北京', 1), ('北京', 1), ('北京', 1), ('上海', 1), ('北京', 1), ('杭州', 1), ('北京', 1), ('北京', 1), ('郑州', 1), ('北京', 1), ('杭州', 1), ('北京', 1), ('北京', 1), ('北京', 1), ('杭州', 1), ('天津', 1), ('北京', 1), ('北京', 1), ('杭州', 1), ('北京', 1), ('北京', 1), ('北京', 1), ('杭州', 1), ('天津', 1), ('北京', 1), ('北京', 1), ('杭州', 1), ('北京', 1), ('杭州', 1)]


# 过滤数据, 只保留北京的数据
beijing_rdd = orders_rdd.filter(lambda d: d['areaName'] == "北京")

# 组合北京 和 商品类型形成新的字符串
category_rdd = beijing_rdd.map(lambda x: x['areaName'] + "_" + x['category'])

# 对结果集进行去重操作
result_rdd = category_rdd.distinct()

# 输出
# print(result_rdd.collect())
# ['北京_平板电脑', '北京_家电', '北京_电脑', '北京_家具', '北京_书籍', '北京_食品', '北京_服饰', '北京_手机']

# 过滤特定类别的订单
# filtered_orders = orders_rdd.filter(lambda order: order['category'] in ['手机', '电脑'])
# print(filtered_orders.collect())
# [{'id': 2, 'timestamp': '2019-05-08T01:01.00Z', 'category': '手机', 'areaName': '北京', 'money': '1450'}, {'id': 3, 'timestamp': '2019-05-08T01:03.00Z', 'category': '手机', 'areaName': '北京', 'money': '8412'}, {'id': 4, 'timestamp': '2019-05-08T05:01.00Z', 'category': '电脑', 'areaName': '上海', 'money': '1513'}, {'id': 6, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '杭州', 'money': '1550'}, {'id': 7, 'timestamp': '2019-05-08T01:03.00Z', 'category': '电脑', 'areaName': '北京', 'money': '5611'}, {'id': 14, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '北京', 'money': '1261'}, {'id': 15, 'timestamp': '2019-05-08T03:03.00Z', 'category': '电脑', 'areaName': '杭州', 'money': '6660'}, {'id': 16, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '天津', 'money': '6660'}, {'id': 19, 'timestamp': '2019-05-08T01:03.00Z', 'category': '电脑', 'areaName': '杭州', 'money': '5551'}, {'id': 20, 'timestamp': '2019-05-08T01:01.00Z', 'category': '电脑', 'areaName': '北京', 'money': '2450'}]

# 按地区统计订单数量
area_order_count = orders_rdd.map(lambda order: (order['areaName'], 1)).reduceByKey(lambda a, b: a + b)
print(area_order_count.collect())
# [('北京', 18), ('杭州', 7), ('天津', 2), ('上海', 1), ('郑州', 1)]


# 按地区统计订单总金额，并按总金额从高到低排序
area_total_money = orders_rdd.map(lambda order: (order['areaName'], float(order['money']))) \
    .reduceByKey(lambda a, b: a + b) \
    .sortBy(lambda x: x[1], ascending=False)

print(area_total_money.collect())
# 按地区统计订单总金额（从高到低）:
# 北京: 91556.00 元
# 杭州: 28831.00 元
# 天津: 12260.00 元
# 上海: 1513.00 元
# 郑州: 1120.00 元

# 按小时统计订单数量
hourly_order_count = orders_rdd.map(lambda order: (datetime.strptime(order['timestamp'], "%Y-%m-%dT%H:%M.%SZ").hour, 1)) \
    .reduceByKey(lambda a, b: a + b)

print(hourly_order_count.collect())
# 按小时统计订单数量:
# Hour 2: 1
# Hour 1: 23
# Hour 5: 3
# Hour 3: 2

# 找出每个地区最贵的订单


# 打印结果
print("按地区统计订单数量:")
for area, count in area_order_count.collect():
    print(f"{area}: {count}")

print("\n按地区统计订单总金额（从高到低）:")
for area, total in area_total_money.collect():
    print(f"{area}: {total:.2f} 元")

print("\n按小时统计订单数量:")
for hour, count in hourly_order_count.collect():
    print(f"Hour {hour}: {count}")


# 停止 SparkContext
sc.stop()




```

## Scala:Basic
### 编写一个循环，将整数数组中相邻的元素置换。比如Array(6, 99, 30, 48, 100, 66, 10, 3)，置换后为Array(99, 6, 48, 30, 66, 100, 3, 10)
```scala
object Main {
  def revert(arr: Array[Int]): Unit = {
    for (i <- 0 until (arr.length - 1, 2)) {
      val t = arr(i)
      arr(i) = arr(i + 1)
      arr(i + 1) = t
    }
  }

  def main(args: Array[String]): Unit = {
    val a = Array(6, 99, 30, 48, 100, 66, 10, 3)
    revert(a)
    println(a.toBuffer)
  }
}
```
### 编写一个函数，将double数组转换为指定列数的二维数组，传入列数作为参数
```scala
def divArr(arr:Array[Double], c:Int) -> Array[Array[Double]]:
    return arr.grouped(c).toArray()

divArr(Array(1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0), 3).foreach(println(_.toBuffer))
```



## SparkSQL（其实就是SQL）
### SparkSQL实现WordCount
#### 核心语句：
```SQL
SELECT word, COUNT(*) AS count
FROM (
    SELECT EXPLODE(SPLIT(line, ' ')) AS word
    FROM lines
) AS words
GROUP BY word
ORDER BY count DESC
```
#### PySpark流程（基于`SparkSession`）：
```python
from pyspark.sql import SparkSession

# 初始化 SparkSession
spark = SparkSession.builder \
    .appName("WordCountWithSparkSQL") \
    .getOrCreate()

# 读取文本文件
file_path = "words.txt"  # 替换为你的文件路径
lines = spark.read.text(file_path)

# 创建 DataFrame
lines_df = lines.withColumnRenamed("value", "line")  # 重命名列为 "line"

# 注册临时表
lines_df.createOrReplaceTempView("lines")

# 使用 Spark SQL 分割单词并统计
word_count_query = """
SELECT word, COUNT(*) AS count
FROM (
    SELECT EXPLODE(SPLIT(line, ' ')) AS word
    FROM lines
) AS words
GROUP BY word
ORDER BY count DESC
"""

# 执行 SQL 查询
word_count_df = spark.sql(word_count_query)

# 显示结果
word_count_df.show()

# 保存结果到文件
output_path = "word_count_result.csv"  # 替换为你希望的输出路径
word_count_df.coalesce(1).write.csv(output_path, header=True)

# 停止 SparkSession
spark.stop()
```
#### Scala流程（基于`import spark.implicits._`）：
```scala
// 导入必需的隐式转换
import spark.implicits._

// 读取文件内容并转换为 Dataset
val lines = spark.read.textFile("words.txt") // 替换为你的文件路径

// 拆分单词，生成 DataFrame
val words = lines
  .flatMap(line => line.split(" ")) // 按空格拆分为单词
  .toDF("word")                     // 将单词转换为 DataFrame，并命名列为 "word"

// 注册为临时表以供 SQL 查询
words.createOrReplaceTempView("words_table")

// 使用 SparkSQL 查询，统计每个单词的出现次数
val wordCounts = spark.sql(
  """
  SELECT word, COUNT(*) AS count
  FROM words_table
  GROUP BY word
  ORDER BY count DESC
  """
)

// 显示结果
wordCounts.show()

// 保存结果到文件
wordCounts.write.csv("word_count_result") // 替换为你的输出路径

```

