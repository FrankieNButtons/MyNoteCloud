##
### 题型
#### 小题(30分)
- 填空题*20（10*2空）
- 选择题*10
#### 大题(70分)
- 计算题*3
- 应用题*3
- 画图题*2
### 大题
- 向前、向后传播的手动计算过程（包括`数值微分`和`反向传播`等两种方法）
- 基于`layers.py`的内容给出计算流程图（计算图）


## 课本回顾
### 课本小结大纲
- Python 基础——P19
    - 基本语法
    - Numpy基础
- 感知机——P36
    - 逻辑门的感知机表示
    - 逻辑门的真值表
    - 与门、或门、非门的参数化表示
    - numpy的shape的含义与解读
    - jacobian行列式的展平与求导方法
- 神经网络——P79
    - 神经网络和感知机的区别（二元输入输出与连续值传播）
    - 常见激活函数的表达式
    - 基本的矩阵运算
    - softmax如何避免上溢和下溢(类似中心化，就是同加同减一个数（一般为几个数里的最大值）然后再softmax)
    - $$y_k=\frac{e^{a_k}}{\sum^{n}_{i=1}e^{a_{i}}}=\frac{Ce^{a_k}}{C\sum^{n}_{i=1}e^{a_{i}}}=\frac{e^{a_k+C'}}{\sum^{n}_{i=1}e^{a_{i}+C'}}{}$$
    - 常见激活函数的导数
    - 模型计算量和参数量的计算
- 神经网络的学习——P118
    - 常见的损失函数
    - 有无监督的区别
    - 深度学习与传统机器学习的区别（特征提取方式）
    - 梯度下降法
    - 数值微分法（包括前向差分、后向差分、中心差分，一般偏好中心差分）
    - 过拟合的概念与部分解决方法
    - 常见的正则化方法
    - 
- 误差的反向传播——P161
    - 链式法则与计算图
    - 误差反向传播
    - 常见激活函数基于计算图的`forward`与`backward`写法
    - 小批量梯度下降
    - 线性层的前向与反向传播数学推导（包括各分量的梯度的表达式）
    - SoftmaxWithCrossEntropyLoss的前向与反向传播推导细节（尤其是是否需要转置）
    - 
- 学习的相关技巧——P200
    - SGD的方法与特点
    - AdaGrad的方法与特点
    - Momentum的方法与特点
    - Adam的方法与特点
    - RMSprop的方法与特点
    - Xaxier初始化的定义与特点
    - He初始化的定义与特点
    - Dropout的方法与特点
    - BatchNorm的方法与特点
    - 数据增强的常见方法
- 卷积神经网络——P201
    - 卷积层的参数及其意义
    - 卷积层处理后shape的推导（含`kernel`, `paddings`, `strides`的计算公式）
  $$h_{\text{new}}=\frac{h_{\text{old}}+2*\text{padding}-kernal}{\text{stride}}+1$$
    - 卷积层的加速与并行计算方法
    - 池化层的参数及其意义
    - 给出卷积核和池化窗口的输出手工计算
    - 卷积网络架构图的绘制
    - padding的策略
        - 0填充（课本实现）
        - 重复填充（重复边缘值）
        - 平铺填充
        - 镜像填充
- 深度学习——P233
    - 加深网络的功臣——ResNet
    - ResNet的残差结构
    - ResNet的思想精髓（学期望变学残差，$F(x)$变$F(x)+x$）
    - U-Net的特点（反卷积的定义）
    - 一些DL的前沿知识